{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84f56c2",
   "metadata": {},
   "source": [
    "# DBpedia Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c47aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46c72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer excel de topics para consultar en la DBpedia\n",
    "data = pd.read_csv('../data_fundamentosdeanalisisdedatos/topicsDbpedia.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9248c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_dbpedia</th>\n",
       "      <th>uri_dbpedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data science concepts</td>\n",
       "      <td>Data_science</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_science&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big data concepts</td>\n",
       "      <td>Big_data</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Big_data&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data mining concepts</td>\n",
       "      <td>Data_mining</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_mining&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine learning algorithms</td>\n",
       "      <td>Machine_learning</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Machine_learning&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>Python_(programming_language)</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Python_(programmi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         topic                  topic_dbpedia  \\\n",
       "0        Data science concepts                   Data_science   \n",
       "1            Big data concepts                       Big_data   \n",
       "2         Data mining concepts                    Data_mining   \n",
       "3  Machine learning algorithms               Machine_learning   \n",
       "4                       Python  Python_(programming_language)   \n",
       "\n",
       "                                         uri_dbpedia  \n",
       "0         <http://dbpedia.org/resource/Data_science>  \n",
       "1             <http://dbpedia.org/resource/Big_data>  \n",
       "2          <http://dbpedia.org/resource/Data_mining>  \n",
       "3     <http://dbpedia.org/resource/Machine_learning>  \n",
       "4  <http://dbpedia.org/resource/Python_(programmi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551b5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62 entries, 0 to 61\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   topic          58 non-null     object\n",
      " 1   topic_dbpedia  58 non-null     object\n",
      " 2   uri_dbpedia    58 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a13a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc3b144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic            4\n",
       "topic_dbpedia    4\n",
       "uri_dbpedia      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# revisar nulos por la transformacion a csv\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb65cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e0881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic            0\n",
       "topic_dbpedia    0\n",
       "uri_dbpedia      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7179d5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfc0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar la API de consulta de SPARQL de DBpedia\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af86600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer del df los dbr que vamos a buscar en la dbpedia\n",
    "uri = data['uri_dbpedia'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1dd794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<http://dbpedia.org/resource/Data_science>',\n",
       " '<http://dbpedia.org/resource/Big_data>',\n",
       " '<http://dbpedia.org/resource/Data_mining>',\n",
       " '<http://dbpedia.org/resource/Machine_learning>',\n",
       " '<http://dbpedia.org/resource/Python_(programming_language)>',\n",
       " '<http://dbpedia.org/resource/R_(programming_language)>',\n",
       " '<http://dbpedia.org/resource/Project_Jupyter>',\n",
       " '<http://dbpedia.org/resource/Apache_Spark>',\n",
       " '<http://dbpedia.org/resource/Kaggle>',\n",
       " '<http://dbpedia.org/resource/GitHub>',\n",
       " '<http://dbpedia.org/resource/Data_extraction>',\n",
       " '<http://dbpedia.org/resource/Web_API>',\n",
       " '<http://dbpedia.org/resource/Scrapy>',\n",
       " '<http://dbpedia.org/resource/Data_storage>',\n",
       " '<http://dbpedia.org/resource/Data_quality>',\n",
       " '<http://dbpedia.org/resource/Data_management>',\n",
       " '<http://dbpedia.org/resource/Data_manipulation_language>',\n",
       " '<http://dbpedia.org/resource/Comma-separated_values>',\n",
       " '<http://dbpedia.org/resource/JSON>',\n",
       " '<http://dbpedia.org/resource/Text_file>',\n",
       " '<http://dbpedia.org/resource/Data_type>',\n",
       " '<http://dbpedia.org/resource/Data_pre-processing>',\n",
       " '<http://dbpedia.org/resource/Data_cleansing>',\n",
       " '<http://dbpedia.org/resource/Missing_data>',\n",
       " '<http://dbpedia.org/resource/Outlier>',\n",
       " '<http://dbpedia.org/resource/Data_transformation>',\n",
       " '<http://dbpedia.org/resource/Dimensionality_reduction>',\n",
       " '<http://dbpedia.org/resource/Principal_component_analysis>',\n",
       " '<http://dbpedia.org/resource/Exploratory_data_analysis>',\n",
       " '<http://dbpedia.org/resource/Categorical_variable>',\n",
       " '<http://dbpedia.org/resource/Univariate_analysis>',\n",
       " '<http://dbpedia.org/resource/Multivariate_statistics>',\n",
       " '<http://dbpedia.org/resource/Regression>',\n",
       " '<http://dbpedia.org/resource/Linear_regression>',\n",
       " '<http://dbpedia.org/resource/Simple_linear_regression>',\n",
       " '<http://dbpedia.org/resource/Regression_analysis>',\n",
       " '<http://dbpedia.org/resource/Nonlinear_regression>',\n",
       " '<http://dbpedia.org/resource/Decision_tree>',\n",
       " '<http://dbpedia.org/resource/Neural_network>',\n",
       " '<http://dbpedia.org/resource/Correlation>',\n",
       " '<http://dbpedia.org/resource/Correspondence_analysis>',\n",
       " '<http://dbpedia.org/resource/Quantitative_Descriptive_Analysis>',\n",
       " '<http://dbpedia.org/resource/Central_tendency>',\n",
       " '<http://dbpedia.org/resource/Statistical_dispersion>',\n",
       " '<http://dbpedia.org/resource/Frequency_(statistics)>',\n",
       " '<http://dbpedia.org/resource/Variance>',\n",
       " '<http://dbpedia.org/resource/Factor_analysis>',\n",
       " '<http://dbpedia.org/resource/Quartile>',\n",
       " '<http://dbpedia.org/resource/Predictive_analytics>',\n",
       " '<http://dbpedia.org/resource/Decision_tree>',\n",
       " '<http://dbpedia.org/resource/Neural_network>',\n",
       " '<http://dbpedia.org/resource/K-nearest_neighbors_algorithm>',\n",
       " '<http://dbpedia.org/resource/Statistical_inference>',\n",
       " '<http://dbpedia.org/resource/Data_and_information_visualization>',\n",
       " '<http://dbpedia.org/resource/Microsoft_Power_BI>',\n",
       " '<http://dbpedia.org/resource/Tableau>',\n",
       " '<http://dbpedia.org/resource/Matplotlib>',\n",
       " '<http://dbpedia.org/resource/Plotly>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8366e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint de la API de DBPedia\n",
    "endpoint_url = \"http://dbpedia.org/sparql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb80fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta SPARQL para extraer la descripción y la imagen de un recurso\n",
    "query = \"\"\"\n",
    "SELECT ?description ?image\n",
    "WHERE {\n",
    "  %s dbo:abstract ?description .\n",
    "  FILTER (langMatches(lang(?description), \"en\"))\n",
    "  OPTIONAL {\n",
    "    %s dbo:thumbnail ?image .\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf237ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar listas para almacenar descripciones e imágenes\n",
    "descriptions = []\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e58440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer la consulta para cada recurso y agregar la descripción y la imagen a las listas\n",
    "for u in uri:\n",
    "    # Formatear la consulta con el recurso actual\n",
    "    formatted_query = query % (u, u)\n",
    "    \n",
    "    # Inicializar objeto SPARQLWrapper y establecer el endpoint y la consulta\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(formatted_query)\n",
    "    \n",
    "    # Especificar el formato de salida y ejecutar la consulta\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Obtener la descripción e imagen si existen y agregarlas a las listas\n",
    "    if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "        description = results[\"results\"][\"bindings\"][0][\"description\"][\"value\"]\n",
    "        descriptions.append(description)\n",
    "        if \"image\" in results[\"results\"][\"bindings\"][0]:\n",
    "            image = results[\"results\"][\"bindings\"][0][\"image\"][\"value\"]\n",
    "            images.append(image)\n",
    "        else:\n",
    "            images.append('')\n",
    "    else:\n",
    "        # Manejar el caso en que no existe una descripción para el recurso\n",
    "        descriptions.append('')\n",
    "        images.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "567ccf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains. Data science is related to data mining, machine learning and big data. Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyse actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge. A data scientist is someone who creates programming code and combines it with statistical knowledge to create insights from data.', 'Big data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data. Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"Analysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research. The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization. Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"', 'Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate. The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.', \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. In its application across business problems, machine learning is also referred to as predictive analytics.\", 'Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020. Python consistently ranks as one of the most popular programming languages.', 'R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software. Users have created packages to augment the functions of the R language. According to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages used in data mining. As of March 2022, R ranks 11th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020. The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface. Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.', \"Project Jupyter (/ˈdʒuːpɪtər/) is a project with goals to develop open-source software, open standards, and services for interactive computing across multiple programming languages. It was spun off from IPython in 2014 by Fernando Pérez and Brian Granger. Project Jupyter's name is a reference to the three core programming languages supported by Jupyter, which are Julia, Python and R. Its name and logo are an homage to Galileo's discovery of the moons of Jupiter, as documented in notebooks attributed to Galileo. Project Jupyter has developed and supported the interactive computing products Jupyter Notebook, JupyterHub, and JupyterLab. Jupyter is financially sponsored by NumFOCUS.\", \"Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\", 'Kaggle, a subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges. Kaggle got its start in 2010 by offering machine learning competitions and now also offers a public data platform, a cloud-based workbench for data science, and Artificial Intelligence education. Its key personnel were Anthony Goldbloom and Jeremy Howard. Nicholas Gruen was the founding chair succeeded by Max Levchin. Equity was raised in 2011 valuing the company at $25.2 million. On 8 March 2017, Google announced that they were acquiring Kaggle.', 'GitHub, Inc., is an Internet hosting service for software development and version control using Git. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project. Headquartered in California, it has been a subsidiary of Microsoft since 2018. It is commonly used to host open source software development projects. As of June 2022, GitHub reported having over 83 million developers and more than 200 million repositories, including at least 28 million public repositories. It is the largest source code host as of November 2021.', \"Data extraction is the act or process of retrieving data out of (usually unstructured or poorly structured) data sources for further data processing or data storage (data migration). The import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to export to another stage in the data workflow. Usually, the term data extraction is applied when (experimental) data is first imported into a computer from primary sources, like measuring or recording devices. Today's electronic devices will usually present an electrical connector (e.g. USB) through which 'raw data' can be streamed into a personal computer.\", \"A web API is an application programming interface for either a web server or a web browser. It is a web development concept, usually limited to a web application's client-side (including any web frameworks being used), and thus usually does not include web server or browser implementation details such as SAPIs or APIs unless publicly accessible by a remote web application.\", 'Scrapy (/ˈskreɪpaɪ/ SKRAY-peye) is a free and open-source web-crawling framework written in Python. Originally designed for web scraping, it can also be used to extract data using APIs or as a general-purpose web crawler. It is currently maintained by , a web-scraping development and services company. Scrapy project architecture is built around \"spiders\", which are self-contained crawlers that are given a set of instructions. Following the spirit of other don\\'t repeat yourself frameworks, such as Django, it makes it easier to build and scale large crawling projects by allowing developers to reuse their code. The Scrapy framework provides you with powerful features such as auto-throttle, rotating proxies and user-agents, allowing you scrape virtually undetected across the net. Scrapy also provides a web-crawling shell, which can be used by developers to test their assumptions on a site’s behavior. Some well-known companies and products using Scrapy are: Lyst, Parse.ly, , Sciences Po Medialab, Data.gov.uk’s World Government Data site.[1]', 'Data storage is the recording (storing) of information (data) in a storage medium. Handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media. Biological media include RNA and DNA. Recording may be accomplished with virtually any form of energy. Electronic data storage requires electrical power to store and retrieve data. Data storage in a digital, machine-readable medium is sometimes called digital data. Computer data storage is one of the core functions of a general-purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.', 'Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People\\'s views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.', 'Data management comprises all disciplines related to managing data as a valuable resource.', 'A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language. Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing. A popular data manipulation language is that of Structured Query Language (SQL), which is used to retrieve and manipulate data in a relational database. Other forms of DML are those used by IMS/DLI, CODASYL databases, such as IDMS and others.', 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields. The CSV file format is not fully standardized. Separating fields with commas is the foundation, but commas in the data or embedded line breaks have to be handled specially. Some implementations disallow such content while others surround the field with quotation marks, which yet again creates the need for escaping if quotation marks are present in the data. The term \"CSV\" also denotes several closely-related delimiter-separated formats that use other field delimiters such as semicolons. These include tab-separated values and space-separated values. A delimiter guaranteed not to be part of the data greatly simplifies parsing. Alternative delimiter-separated files are often given a \".csv\" extension despite the use of a non-comma field separator. This loose terminology can cause problems in data exchange. Many applications that accept CSV files have options to select the delimiter character and the quotation character. Semicolons are often used instead of commas in many European locales in order to use the comma as the decimal separator and, possibly, the period as a decimal grouping character.', 'JSON (JavaScript Object Notation, pronounced /ˈdʒeɪsən/; also /ˈdʒeɪˌsɒn/) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values). It is a common data format with diverse uses in electronic data interchange, including that of web applications with servers. JSON is a language-independent data format. It was derived from JavaScript, but many modern programming languages include code to generate and parse JSON-format data. JSON filenames use the extension .json. Douglas Crockford originally specified the JSON format in the early 2000s. He and Chip Morningstar sent the first JSON message in April 2001.', 'A text file (sometimes spelled textfile; an old alternative name is flatfile) is a kind of computer file that is structured as a sequence of lines of electronic text. A text file exists stored as data within a computer file system. In operating systems such as CP/M and MS-DOS, where the operating system does not keep track of the file size in bytes, the end of a text file is denoted by placing one or more special characters, known as an end-of-file marker, as padding after the last line in a text file. On modern operating systems such as Microsoft Windows and Unix-like systems, text files do not contain any special EOF character, because file systems on those operating systems keep track of the file size in bytes. Most text files need to have end-of-line delimiters, which are done in a few different ways depending on operating system. Some operating systems with record-orientated file systems may not use new line delimiters and will primarily store text files with lines separated as fixed or variable length records. \"Text file\" refers to a type of container, while plain text refers to a type of content. At a generic level of description, there are two kinds of computer files: text files and binary files.', 'In computer science and computer programming, a data type (or simply type) is a set of possible values and a set of allowed operations on it. A data type tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the possible values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored.', 'Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running any analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Examples of data preprocessing include cleaning, instance selection, normalization, one hot encoding, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set. Data preprocessing may affect the way in which outcomes of the final data processing can be interpreted. This aspect should be carefully considered when interpretation of the results is a key point, such in the multivariate processing of chemical data (chemometrics).', 'Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall. After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data. The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of \"varying file formats, naming conventions, and columns\", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (\"st, rd, etc.\" to \"street, road, etcetera\").', 'In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Missing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (\"subject\"). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies—for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing. Data often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher—for example, when data collection is done improperly or mistakes are made in data entry. These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random. Missing data can be handled similarly as censored data.', 'In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate \\'correct trial\\' versus \\'measurement error\\'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition). Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 °C, the median of the data will be between 20 and 25 °C but the mean temperature will be between 35.5 and 40 °C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as \"a typical sample\", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set. Estimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.', '', 'Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.', \"Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as Population Genetics, Microbiome studies, Atmospheric Science etc. The principal components of a collection of points in a real coordinate space are a sequence of unit vectors, where the -th vector is the direction of a line that best fits the data while being orthogonal to the first vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. In data analysis, the first principal component of a set of variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set. PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The -th principal component can be taken as a direction orthogonal to the first principal components that maximizes the variance of the projected data. For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.\", 'In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.', 'In statistics, a categorical variable (also called qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from observations made of qualitative data that are summarised as counts or cross tabulations, or from observations of quantitative data grouped within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term \"categorical data\" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables. A categorical variable that can take on exactly two values is termed a binary variable or a dichotomous variable; an important special case is the Bernoulli variable. Categorical variables with more than two possible values are called polytomous variables; categorical variables are often assumed to be polytomous unless otherwise specified. Discretization is treating continuous data as if it were categorical. Dichotomization is treating continuous data or polytomous variables as if they were binary variables. Regression analysis often treats category membership with one or more quantitative dummy variables.', 'Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.', 'Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied. In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both \\n* how these can be used to represent the distributions of observed data; \\n* how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis. Certain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.', '', 'In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine. Linear regression has many practical uses. Most applications fall into one of the following two broad categories: \\n* If the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response. \\n* If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response. Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.', 'In statistics, simple linear regression is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variable.The adjective simple refers to the fact that the outcome variable is related to a single predictor. It is common to make the additional stipulation that the ordinary least squares (OLS) method should be used: the accuracy of each predicted value is measured by its squared residual (vertical distance between the point of the data set and the fitted line), and the goal is to make the sum of these squared deviations as small as possible. Other regression methods that can be used in place of ordinary least squares include least absolute deviations (minimizing the sum of absolute values of residuals) and the Theil–Sen estimator (which chooses a line whose slope is the median of the slopes determined by pairs of sample points). Deming regression (total least squares) also finds a line that fits a set of two-dimensional sample points, but (unlike ordinary least squares, least absolute deviations, and median slope regression) it is not really an instance of simple linear regression, because it does not separate the coordinates into one dependent and one independent variable and could potentially return a vertical line as its fit. The remainder of the article assumes an ordinary least squares regression.In this case, the slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that the line passes through the center of mass (x, y) of the data points.', \"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression). Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using observational data.\", 'In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.', 'A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.', 'A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1. These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.', 'In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it normally refers to the degree to which a pair of variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation). Formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence. In informal parlance, correlation is synonymous with dependence. However, when used in a technical sense, correlation refers to any of several specific types of mathematical operations between the tested variables and their respective expected values. Essentially, correlation is the measure of how two or more variables are related to one another. There are several correlation coefficients, often denoted or , measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). Other correlation coefficients – such as Spearman\\'s rank correlation – have been developed to be more robust than Pearson\\'s, that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables.', 'Correspondence analysis (CA) is a multivariate statistical technique proposed by Herman Otto Hartley (Hirschfeld) and later developed by Jean-Paul Benzécri. It is conceptually similar to principal component analysis, but applies to categorical rather than continuous data. In a similar manner to principal component analysis, it provides a means of displaying or summarising a set of data in two-dimensional graphical form. Its aim is to display in a biplot any structure hidden in the multivariate setting of the data table. As such it is a technique from the field of multivariate ordination. Since the variant of CA described here can be applied either with a focus on the rows or on the columns it should in fact be called simple (symmetric) correspondence analysis. It is traditionally applied to the contingency table of a pair of nominal variables where each cell contains either a count or a zero value. If more than two categorical variables are to be summarized, a variant called multiple correspondence analysis should be chosen instead. CA may also be applied to binary data given the presence/absence coding represents simplified count data i.e. a 1 describes a positive count and 0 stands for a count of zero. Depending on the scores used CA preserves the chi-square distance between either the rows or the columns of the table. Because CA is a descriptive technique, it can be applied to tables regardless of a significant chisquared test. Although the statistic used in inferential statistics and the chi-square distance are computationally related they should not be confused since the latter works as a multivariate statistical distance measure in CA while the statistic is in fact a scalar not a metric.', 'Developed by in 1974, Quantitative Descriptive Analysis (QDA) is a behavioral sensory evaluation approach that uses descriptive panels to measure a product’s sensory characteristics.Panel members use their senses to identify perceived similarities and differences in products, and articulate those perceptions in their own words.Sensory evaluation is a science that measures, analyzes, and interprets the reactions of the senses of sight, smell, sound, taste, and texture (or kinesthesis) to products. It is a people science; i.e., people are essential to obtain information about products. Tragon QDA is a registered trademark with the United States Patent and Trademark Office.The term was coined by Herbert Stone (a food scientist) and Joel L. Sidel (a psychologist)in 1974 while at the Stanford Research Institute, (now known as ).Stone and Sidel later founded Tragon Corporation, a successful spin-off of SRI, to develop and market QDA. Originally developed within the food industry, QDA is the basis of many disciplines that involve the senses, such as clothing, cosmetics, and electronics.', 'In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s. The most common measures of central tendency are the arithmetic mean, the median, and the mode. A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote \"the tendency of quantitative data to cluster around some central value.\" The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.', 'In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range. For instance, when the variance of data in a set is large, the data is widely scattered. On the other hand, when the variance is small, the data in the set is clustered. Dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.', 'In statistics, the frequency (or absolute frequency) of an event is the number of times the observation occurred/recorded in an experiment or study. These frequencies are often depicted graphically or in tabular form.', 'In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its population mean or sample mean. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by , , , , or . An advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation; for example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances. A disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished. There are two distinct concepts that are both called \"variance\". One, as discussed above, is part of a theoretical probability distribution and is defined by an equation. The other variance is a characteristic of a set of observations. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below. The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution\\'s equation for variance.', 'Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models. Simply put, the factor loading of a variable quantifies the extent to which the variable is related to a given factor. A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.', 'In statistics, a quartile is a type of quantile which divides the number of data points into four parts, or quarters, of more-or-less equal size. The data must be ordered from smallest to largest to compute quartiles; as such, quartiles are a form of order statistic. The three main quartiles are as follows: \\n* The first quartile (Q1) is defined as the middle number between the smallest number (minimum) and the median of the data set. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. \\n* The second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point. \\n* The third quartile (Q3) is the middle value between the median and the highest value (maximum) of the data set. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Along with the minimum and maximum of the data (which are also quartiles), the three quartiles described above provide a five-number summary of the data. This summary is important in statistics because it provides information about both the center and the spread of the data. Knowing the lower and upper quartile provides information on how big the spread is and if the dataset is skewed toward one side. Since quartiles divide the number of data points evenly, the range is not the same between quartiles (i.e., Q3-Q2 ≠ Q2-Q1) and is instead known as the interquartile range (IQR). While the maximum and minimum also show the spread of the data, the upper and lower quartiles can provide more detailed information on the location of specific data points, the presence of outliers in the data, and the difference in spread between the middle 50% of the data and the outer data points.', 'Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modeling, and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions. The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.', 'A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.', 'A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1. These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.', 'In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression: \\n* In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \\n* In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically. Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.', 'Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean \"make a prediction, by evaluating an already trained model\"; in this context inferring properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.', \"Data and information visualization (data viz or info viz) is an interdisciplinary field that deals with the graphic representation of data and information. It is a particularly efficient way of communicating when the data or information is numerous as for example a time series. It is also the study of visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. It is related to infographics and scientific visualization. One distinction is that it's information visualization when the spatial representation (e.g., the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given. From an academic point of view, this representation can be considered as a mapping between the original data (usually numerical) and graphic elements (for example, lines or points in a chart). The mapping determines how the attributes of these elements vary according to the data. In this light, a bar chart is a mapping of the length of a bar to a magnitude of a variable. Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization. Data and information visualization has its roots in the field of statistics and is therefore generally considered a branch of descriptive statistics. However, because both design skills and statistical and computing skills are required to visualize effectively, it is argued by authors such as Gershon and Page that it is both an art and a science. Research into how people read and misread various types of visualizations is helping to determine what types and features of visualizations are most understandable and effective in conveying information.\", 'Power BI is an interactive data visualization software product developed by Microsoft with a primary focus on business intelligence. It is part of the Microsoft Power Platform.Power BI is a collection of software services, apps, and connectors that work together to turn unrelated sources of data into coherent, visually immersive, and interactive insights. Data may be input by reading directly from a database, webpage, or structured files such as spreadsheets, CSV, XML, and JSON.', '', 'Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. There is also a procedural \"pylab\" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. SciPy makes use of Matplotlib. Matplotlib was originally written by John D. Hunter. Since then it has had an active development community and is distributed under a BSD-style license. Michael Droettboom was nominated as matplotlib\\'s lead developer shortly before John Hunter\\'s death in August 2012 and was further joined by Thomas Caswell. Matplotlib is a fiscally sponsored project. Matplotlib 2.0.x supports Python versions 2.7 through 3.10. Python 3 support started with Matplotlib 1.2. Matplotlib 1.4 is the last version to support Python 2.6. Matplotlib has pledged not to support Python 2 past 2020 by signing the Python 3 Statement.', 'Plotly is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, and REST.']\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las listas de descripciones e imágenes\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cee910a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://commons.wikimedia.org/wiki/Special:FilePath/PIA23792-1600x1200(1).jpg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Hilbert_InfoGrowth.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Spurious_correlations_-_spelling_bee_spiders.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/AI_hierarchy.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Python-logo-notext.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/R_logo.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Jupyter_logo.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Apache_Spark_logo.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Kaggle_logo.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/GitHub_logo_2013.svg?width=300', '', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Scrapy_logo.jpg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/RNA-comparedto-DNA_thymineAndUracilCorrected.png?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Data_lifecycle.svg?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/CsvDelimited001.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/JSON_vector_logo.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Text-txt.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Python_3._The_standard_type_hierarchy.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/SimpleSemanticDataMiningDiagram.png?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Missing_not_at_random.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Michelsonmorley-boxplot.svg?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/PCA_Projection_Illustration.gif?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/GaussianScatterPCA.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Data_visualization_process_v1.png?width=300', '', '', '', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Linear_least_squares_example2.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Okuns_law_quarterly_differences.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Normdist_regression.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Michaelis-Menten_saturation_curve_of_an_enzyme_reaction.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Manual_decision_tree.jpg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Neural_network_example.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Correlation_examples2.svg?width=300', '', '', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Comparison_standard_deviations.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Travel_time_histogram_total_n_Stata.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Comparison_standard_deviations.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/FactorPlot.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Boxplot_vs_PDF.svg?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Manual_decision_tree.jpg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Neural_network_example.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/KnnClassification.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Normality_Histogram.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Data_visualization_process_v1.png?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Power_BI_logo.svg?width=300', '', 'http://commons.wikimedia.org/wiki/Special:FilePath/Matplotlib_logo.svg?width=300', 'http://commons.wikimedia.org/wiki/Special:FilePath/Plotly-logo.png?width=300']\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d395c838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "535b727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df83326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame a partir de la lista descriptions\n",
    "new_df = pd.DataFrame({'descriptions': descriptions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a122675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar DataFrame existente y nuevo DataFrame a lo largo del eje de columnas\n",
    "data = pd.concat([data, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47ce49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame a partir de la lista images\n",
    "new_df2 = pd.DataFrame({'images': images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7411bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar DataFrame existente y nuevo DataFrame a lo largo del eje de columnas\n",
    "data = pd.concat([data, new_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4c62c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_dbpedia</th>\n",
       "      <th>uri_dbpedia</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data science concepts</td>\n",
       "      <td>Data_science</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_science&gt;</td>\n",
       "      <td>Data science is an interdisciplinary field tha...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big data concepts</td>\n",
       "      <td>Big_data</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Big_data&gt;</td>\n",
       "      <td>Big data refers to data sets that are too larg...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data mining concepts</td>\n",
       "      <td>Data_mining</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_mining&gt;</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine learning algorithms</td>\n",
       "      <td>Machine_learning</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Machine_learning&gt;</td>\n",
       "      <td>Machine learning (ML) is a field of inquiry de...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>Python_(programming_language)</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Python_(programmi...</td>\n",
       "      <td>Python is a high-level, general-purpose progra...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R</td>\n",
       "      <td>R_(programming_language)</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/R_(programming_la...</td>\n",
       "      <td>R is a programming language for statistical co...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jupyter notebook</td>\n",
       "      <td>Project_Jupyter</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Project_Jupyter&gt;</td>\n",
       "      <td>Project Jupyter (/ˈdʒuːpɪtər/) is a project wi...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apache spark</td>\n",
       "      <td>Apache_Spark</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Apache_Spark&gt;</td>\n",
       "      <td>Apache Spark is an open-source unified analyti...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kaggle</td>\n",
       "      <td>Kaggle</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Kaggle&gt;</td>\n",
       "      <td>Kaggle, a subsidiary of Google LLC, is an onli...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Github</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/GitHub&gt;</td>\n",
       "      <td>GitHub, Inc., is an Internet hosting service f...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data extraction</td>\n",
       "      <td>Data_extraction</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_extraction&gt;</td>\n",
       "      <td>Data extraction is the act or process of retri...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>API</td>\n",
       "      <td>Web_API</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Web_API&gt;</td>\n",
       "      <td>A web API is an application programming interf...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Scrapy</td>\n",
       "      <td>Scrapy</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Scrapy&gt;</td>\n",
       "      <td>Scrapy (/ˈskreɪpaɪ/ SKRAY-peye) is a free and ...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Storage</td>\n",
       "      <td>Data_storage</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_storage&gt;</td>\n",
       "      <td>Data storage is the recording (storing) of inf...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data quality</td>\n",
       "      <td>Data_quality</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_quality&gt;</td>\n",
       "      <td>Data quality refers to the state of qualitativ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data management</td>\n",
       "      <td>Data_management</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_management&gt;</td>\n",
       "      <td>Data management comprises all disciplines rela...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data manipulation</td>\n",
       "      <td>Data_manipulation_language</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_manipulation...</td>\n",
       "      <td>A data manipulation language (DML) is a comput...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Csv</td>\n",
       "      <td>Comma-separated_values</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Comma-separated_v...</td>\n",
       "      <td>A comma-separated values (CSV) file is a delim...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Json</td>\n",
       "      <td>JSON</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/JSON&gt;</td>\n",
       "      <td>JSON (JavaScript Object Notation, pronounced /...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Txt</td>\n",
       "      <td>Text_file</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Text_file&gt;</td>\n",
       "      <td>A text file (sometimes spelled textfile; an ol...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data types</td>\n",
       "      <td>Data_type</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_type&gt;</td>\n",
       "      <td>In computer science and computer programming, ...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data preprocessing</td>\n",
       "      <td>Data_pre-processing</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_pre-processing&gt;</td>\n",
       "      <td>Data preprocessing can refer to manipulation o...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data cleaning</td>\n",
       "      <td>Data_cleansing</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_cleansing&gt;</td>\n",
       "      <td>Data cleansing or data cleaning is the process...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missing data</td>\n",
       "      <td>Missing_data</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Missing_data&gt;</td>\n",
       "      <td>In statistics, missing data, or missing values...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Outliers</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Outlier&gt;</td>\n",
       "      <td>In statistics, an outlier is a data point that...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data transformation</td>\n",
       "      <td>Data_transformation</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_transformation&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dimensional reduction</td>\n",
       "      <td>Dimensionality_reduction</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Dimensionality_re...</td>\n",
       "      <td>Dimensionality reduction, or dimension reducti...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Principal Component Analysis (PCA)</td>\n",
       "      <td>Principal_component_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Principal_compone...</td>\n",
       "      <td>Principal component analysis (PCA) is a popula...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Exploratory data analysis</td>\n",
       "      <td>Exploratory_data_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Exploratory_data_...</td>\n",
       "      <td>In statistics, exploratory data analysis (EDA)...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Categorical variables</td>\n",
       "      <td>Categorical_variable</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Categorical_varia...</td>\n",
       "      <td>In statistics, a categorical variable (also ca...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Univariate data analysis</td>\n",
       "      <td>Univariate_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Univariate_analysis&gt;</td>\n",
       "      <td>Univariate analysis is perhaps the simplest fo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Multivariate data analysis</td>\n",
       "      <td>Multivariate_statistics</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Multivariate_stat...</td>\n",
       "      <td>Multivariate statistics is a subdivision of st...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Regression</td>\n",
       "      <td>Regression</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Regression&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Linear regression</td>\n",
       "      <td>Linear_regression</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Linear_regression&gt;</td>\n",
       "      <td>In statistics, linear regression is a linear a...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Simple linear regression</td>\n",
       "      <td>Simple_linear_regression</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Simple_linear_reg...</td>\n",
       "      <td>In statistics, simple linear regression is a l...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Multiple Linear Regression</td>\n",
       "      <td>Regression_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Regression_analysis&gt;</td>\n",
       "      <td>In statistical modeling, regression analysis i...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Nonlinear regression</td>\n",
       "      <td>Nonlinear_regression</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Nonlinear_regress...</td>\n",
       "      <td>In statistics, nonlinear regression is a form ...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Decision_tree&gt;</td>\n",
       "      <td>A decision tree is a decision support tool tha...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Neural network</td>\n",
       "      <td>Neural_network</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Neural_network&gt;</td>\n",
       "      <td>A neural network is a network or circuit of bi...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Correlation</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Correlation&gt;</td>\n",
       "      <td>In statistics, correlation or dependence is an...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Correspondence analysis</td>\n",
       "      <td>Correspondence_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Correspondence_an...</td>\n",
       "      <td>Correspondence analysis (CA) is a multivariate...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Descriptive analysis</td>\n",
       "      <td>Quantitative_Descriptive_Analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Quantitative_Desc...</td>\n",
       "      <td>Developed by in 1974, Quantitative Descriptive...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Central tendency</td>\n",
       "      <td>Central_tendency</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Central_tendency&gt;</td>\n",
       "      <td>In statistics, a central tendency (or measure ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Dispersion</td>\n",
       "      <td>Statistical_dispersion</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Statistical_dispe...</td>\n",
       "      <td>In statistics, dispersion (also called variabi...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Frequency</td>\n",
       "      <td>Frequency_(statistics)</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Frequency_(statis...</td>\n",
       "      <td>In statistics, the frequency (or absolute freq...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Variance</td>\n",
       "      <td>Variance</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Variance&gt;</td>\n",
       "      <td>In probability theory and statistics, variance...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Factor analysis</td>\n",
       "      <td>Factor_analysis</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Factor_analysis&gt;</td>\n",
       "      <td>Factor analysis is a statistical method used t...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Quartile analysis</td>\n",
       "      <td>Quartile</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Quartile&gt;</td>\n",
       "      <td>In statistics, a quartile is a type of quantil...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Predictive analytics</td>\n",
       "      <td>Predictive_analytics</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Predictive_analyt...</td>\n",
       "      <td>Predictive analytics encompasses a variety of ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Decision_tree&gt;</td>\n",
       "      <td>A decision tree is a decision support tool tha...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Neural network</td>\n",
       "      <td>Neural_network</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Neural_network&gt;</td>\n",
       "      <td>A neural network is a network or circuit of bi...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>K-nearest_neighbors_algorithm</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/K-nearest_neighbo...</td>\n",
       "      <td>In statistics, the k-nearest neighbors algorit...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Inferential analysis</td>\n",
       "      <td>Statistical_inference</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Statistical_infer...</td>\n",
       "      <td>Statistical inference is the process of using ...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Data visualization</td>\n",
       "      <td>Data_and_information_visualization</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Data_and_informat...</td>\n",
       "      <td>Data and information visualization (data viz o...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Microsoft Power BI</td>\n",
       "      <td>Microsoft_Power_BI</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Microsoft_Power_BI&gt;</td>\n",
       "      <td>Power BI is an interactive data visualization ...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tableau</td>\n",
       "      <td>Tableau</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Tableau&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Matplotlib</td>\n",
       "      <td>Matplotlib</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Matplotlib&gt;</td>\n",
       "      <td>Matplotlib is a plotting library for the Pytho...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Plotly</td>\n",
       "      <td>Plotly</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Plotly&gt;</td>\n",
       "      <td>Plotly is a technical computing company headqu...</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 topic                       topic_dbpedia  \\\n",
       "0                Data science concepts                        Data_science   \n",
       "1                    Big data concepts                            Big_data   \n",
       "2                 Data mining concepts                         Data_mining   \n",
       "3          Machine learning algorithms                    Machine_learning   \n",
       "4                               Python       Python_(programming_language)   \n",
       "5                                    R            R_(programming_language)   \n",
       "6                     Jupyter notebook                     Project_Jupyter   \n",
       "7                         Apache spark                        Apache_Spark   \n",
       "8                               Kaggle                              Kaggle   \n",
       "9                               Github                              GitHub   \n",
       "10                     Data extraction                     Data_extraction   \n",
       "11                                 API                             Web_API   \n",
       "12                              Scrapy                              Scrapy   \n",
       "13                        Data Storage                        Data_storage   \n",
       "14                        Data quality                        Data_quality   \n",
       "15                     Data management                     Data_management   \n",
       "16                   Data manipulation          Data_manipulation_language   \n",
       "17                                 Csv              Comma-separated_values   \n",
       "18                                Json                                JSON   \n",
       "19                                 Txt                           Text_file   \n",
       "20                          Data types                           Data_type   \n",
       "21                  Data preprocessing                 Data_pre-processing   \n",
       "22                       Data cleaning                      Data_cleansing   \n",
       "23                        Missing data                        Missing_data   \n",
       "24                            Outliers                             Outlier   \n",
       "25                 Data transformation                 Data_transformation   \n",
       "26               Dimensional reduction            Dimensionality_reduction   \n",
       "27  Principal Component Analysis (PCA)        Principal_component_analysis   \n",
       "28           Exploratory data analysis           Exploratory_data_analysis   \n",
       "29               Categorical variables                Categorical_variable   \n",
       "30            Univariate data analysis                 Univariate_analysis   \n",
       "31          Multivariate data analysis             Multivariate_statistics   \n",
       "32                          Regression                          Regression   \n",
       "33                   Linear regression                   Linear_regression   \n",
       "34            Simple linear regression            Simple_linear_regression   \n",
       "35          Multiple Linear Regression                 Regression_analysis   \n",
       "36                Nonlinear regression                Nonlinear_regression   \n",
       "37                      Decision Trees                       Decision_tree   \n",
       "38                      Neural network                      Neural_network   \n",
       "39                         Correlation                         Correlation   \n",
       "40             Correspondence analysis             Correspondence_analysis   \n",
       "41                Descriptive analysis   Quantitative_Descriptive_Analysis   \n",
       "42                    Central tendency                    Central_tendency   \n",
       "43                          Dispersion              Statistical_dispersion   \n",
       "44                           Frequency              Frequency_(statistics)   \n",
       "45                            Variance                            Variance   \n",
       "46                     Factor analysis                     Factor_analysis   \n",
       "47                   Quartile analysis                            Quartile   \n",
       "48                Predictive analytics                Predictive_analytics   \n",
       "49                      Decision Trees                       Decision_tree   \n",
       "50                      Neural network                      Neural_network   \n",
       "51                   Nearest Neighbors       K-nearest_neighbors_algorithm   \n",
       "52                Inferential analysis               Statistical_inference   \n",
       "53                  Data visualization  Data_and_information_visualization   \n",
       "54                  Microsoft Power BI                  Microsoft_Power_BI   \n",
       "55                             Tableau                             Tableau   \n",
       "56                          Matplotlib                          Matplotlib   \n",
       "57                              Plotly                              Plotly   \n",
       "\n",
       "                                          uri_dbpedia  \\\n",
       "0          <http://dbpedia.org/resource/Data_science>   \n",
       "1              <http://dbpedia.org/resource/Big_data>   \n",
       "2           <http://dbpedia.org/resource/Data_mining>   \n",
       "3      <http://dbpedia.org/resource/Machine_learning>   \n",
       "4   <http://dbpedia.org/resource/Python_(programmi...   \n",
       "5   <http://dbpedia.org/resource/R_(programming_la...   \n",
       "6       <http://dbpedia.org/resource/Project_Jupyter>   \n",
       "7          <http://dbpedia.org/resource/Apache_Spark>   \n",
       "8                <http://dbpedia.org/resource/Kaggle>   \n",
       "9                <http://dbpedia.org/resource/GitHub>   \n",
       "10      <http://dbpedia.org/resource/Data_extraction>   \n",
       "11              <http://dbpedia.org/resource/Web_API>   \n",
       "12               <http://dbpedia.org/resource/Scrapy>   \n",
       "13         <http://dbpedia.org/resource/Data_storage>   \n",
       "14         <http://dbpedia.org/resource/Data_quality>   \n",
       "15      <http://dbpedia.org/resource/Data_management>   \n",
       "16  <http://dbpedia.org/resource/Data_manipulation...   \n",
       "17  <http://dbpedia.org/resource/Comma-separated_v...   \n",
       "18                 <http://dbpedia.org/resource/JSON>   \n",
       "19            <http://dbpedia.org/resource/Text_file>   \n",
       "20            <http://dbpedia.org/resource/Data_type>   \n",
       "21  <http://dbpedia.org/resource/Data_pre-processing>   \n",
       "22       <http://dbpedia.org/resource/Data_cleansing>   \n",
       "23         <http://dbpedia.org/resource/Missing_data>   \n",
       "24              <http://dbpedia.org/resource/Outlier>   \n",
       "25  <http://dbpedia.org/resource/Data_transformation>   \n",
       "26  <http://dbpedia.org/resource/Dimensionality_re...   \n",
       "27  <http://dbpedia.org/resource/Principal_compone...   \n",
       "28  <http://dbpedia.org/resource/Exploratory_data_...   \n",
       "29  <http://dbpedia.org/resource/Categorical_varia...   \n",
       "30  <http://dbpedia.org/resource/Univariate_analysis>   \n",
       "31  <http://dbpedia.org/resource/Multivariate_stat...   \n",
       "32           <http://dbpedia.org/resource/Regression>   \n",
       "33    <http://dbpedia.org/resource/Linear_regression>   \n",
       "34  <http://dbpedia.org/resource/Simple_linear_reg...   \n",
       "35  <http://dbpedia.org/resource/Regression_analysis>   \n",
       "36  <http://dbpedia.org/resource/Nonlinear_regress...   \n",
       "37        <http://dbpedia.org/resource/Decision_tree>   \n",
       "38       <http://dbpedia.org/resource/Neural_network>   \n",
       "39          <http://dbpedia.org/resource/Correlation>   \n",
       "40  <http://dbpedia.org/resource/Correspondence_an...   \n",
       "41  <http://dbpedia.org/resource/Quantitative_Desc...   \n",
       "42     <http://dbpedia.org/resource/Central_tendency>   \n",
       "43  <http://dbpedia.org/resource/Statistical_dispe...   \n",
       "44  <http://dbpedia.org/resource/Frequency_(statis...   \n",
       "45             <http://dbpedia.org/resource/Variance>   \n",
       "46      <http://dbpedia.org/resource/Factor_analysis>   \n",
       "47             <http://dbpedia.org/resource/Quartile>   \n",
       "48  <http://dbpedia.org/resource/Predictive_analyt...   \n",
       "49        <http://dbpedia.org/resource/Decision_tree>   \n",
       "50       <http://dbpedia.org/resource/Neural_network>   \n",
       "51  <http://dbpedia.org/resource/K-nearest_neighbo...   \n",
       "52  <http://dbpedia.org/resource/Statistical_infer...   \n",
       "53  <http://dbpedia.org/resource/Data_and_informat...   \n",
       "54   <http://dbpedia.org/resource/Microsoft_Power_BI>   \n",
       "55              <http://dbpedia.org/resource/Tableau>   \n",
       "56           <http://dbpedia.org/resource/Matplotlib>   \n",
       "57               <http://dbpedia.org/resource/Plotly>   \n",
       "\n",
       "                                         descriptions  \\\n",
       "0   Data science is an interdisciplinary field tha...   \n",
       "1   Big data refers to data sets that are too larg...   \n",
       "2   Data mining is the process of extracting and d...   \n",
       "3   Machine learning (ML) is a field of inquiry de...   \n",
       "4   Python is a high-level, general-purpose progra...   \n",
       "5   R is a programming language for statistical co...   \n",
       "6   Project Jupyter (/ˈdʒuːpɪtər/) is a project wi...   \n",
       "7   Apache Spark is an open-source unified analyti...   \n",
       "8   Kaggle, a subsidiary of Google LLC, is an onli...   \n",
       "9   GitHub, Inc., is an Internet hosting service f...   \n",
       "10  Data extraction is the act or process of retri...   \n",
       "11  A web API is an application programming interf...   \n",
       "12  Scrapy (/ˈskreɪpaɪ/ SKRAY-peye) is a free and ...   \n",
       "13  Data storage is the recording (storing) of inf...   \n",
       "14  Data quality refers to the state of qualitativ...   \n",
       "15  Data management comprises all disciplines rela...   \n",
       "16  A data manipulation language (DML) is a comput...   \n",
       "17  A comma-separated values (CSV) file is a delim...   \n",
       "18  JSON (JavaScript Object Notation, pronounced /...   \n",
       "19  A text file (sometimes spelled textfile; an ol...   \n",
       "20  In computer science and computer programming, ...   \n",
       "21  Data preprocessing can refer to manipulation o...   \n",
       "22  Data cleansing or data cleaning is the process...   \n",
       "23  In statistics, missing data, or missing values...   \n",
       "24  In statistics, an outlier is a data point that...   \n",
       "25                                                      \n",
       "26  Dimensionality reduction, or dimension reducti...   \n",
       "27  Principal component analysis (PCA) is a popula...   \n",
       "28  In statistics, exploratory data analysis (EDA)...   \n",
       "29  In statistics, a categorical variable (also ca...   \n",
       "30  Univariate analysis is perhaps the simplest fo...   \n",
       "31  Multivariate statistics is a subdivision of st...   \n",
       "32                                                      \n",
       "33  In statistics, linear regression is a linear a...   \n",
       "34  In statistics, simple linear regression is a l...   \n",
       "35  In statistical modeling, regression analysis i...   \n",
       "36  In statistics, nonlinear regression is a form ...   \n",
       "37  A decision tree is a decision support tool tha...   \n",
       "38  A neural network is a network or circuit of bi...   \n",
       "39  In statistics, correlation or dependence is an...   \n",
       "40  Correspondence analysis (CA) is a multivariate...   \n",
       "41  Developed by in 1974, Quantitative Descriptive...   \n",
       "42  In statistics, a central tendency (or measure ...   \n",
       "43  In statistics, dispersion (also called variabi...   \n",
       "44  In statistics, the frequency (or absolute freq...   \n",
       "45  In probability theory and statistics, variance...   \n",
       "46  Factor analysis is a statistical method used t...   \n",
       "47  In statistics, a quartile is a type of quantil...   \n",
       "48  Predictive analytics encompasses a variety of ...   \n",
       "49  A decision tree is a decision support tool tha...   \n",
       "50  A neural network is a network or circuit of bi...   \n",
       "51  In statistics, the k-nearest neighbors algorit...   \n",
       "52  Statistical inference is the process of using ...   \n",
       "53  Data and information visualization (data viz o...   \n",
       "54  Power BI is an interactive data visualization ...   \n",
       "55                                                      \n",
       "56  Matplotlib is a plotting library for the Pytho...   \n",
       "57  Plotly is a technical computing company headqu...   \n",
       "\n",
       "                                               images  \n",
       "0   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "1   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "2   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "3   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "4   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "5   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "6   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "7   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "8   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "9   http://commons.wikimedia.org/wiki/Special:File...  \n",
       "10                                                     \n",
       "11                                                     \n",
       "12  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "13  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "14                                                     \n",
       "15  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "16                                                     \n",
       "17  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "18  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "19  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "20  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "21  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "22                                                     \n",
       "23  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "24  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "25                                                     \n",
       "26  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "27  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "28  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "29                                                     \n",
       "30                                                     \n",
       "31                                                     \n",
       "32                                                     \n",
       "33  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "34  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "35  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "36  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "37  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "38  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "39  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "40                                                     \n",
       "41                                                     \n",
       "42                                                     \n",
       "43  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "44  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "45  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "46  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "47  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "48                                                     \n",
       "49  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "50  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "51  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "52  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "53  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "54  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "55                                                     \n",
       "56  http://commons.wikimedia.org/wiki/Special:File...  \n",
       "57  http://commons.wikimedia.org/wiki/Special:File...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspeccionamos la data\n",
    "data.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "995e35f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar csv con todos los metadatos extraidos\n",
    "data.to_csv(\"api_dataExtraction/dbpedia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dce05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
